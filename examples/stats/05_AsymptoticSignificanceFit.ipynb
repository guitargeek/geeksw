{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Combined Fit with the Asymptotic Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will go beyond estimating epected significances with the **approximate median significance** calculated for each bin assuming Poissonian counts, and then adding them in quadrature.\n",
    "\n",
    "For a complete analysis, this approach that sees all bins as uncorrelated is too simplistic and will become invalid as soon as you introduce things like systematic uncertainties which are correlated between the bins.\n",
    "\n",
    "Therefore, we need to set up a likelihood-based fitting framework which is a bit more flexible. If you are curious how the CMS collaboration does this, keep in mind that it has it's own framework called \"[Higgs combine](https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit)\". We won't use it because it is a big \"black box\" and does not have much pedagogical value.\n",
    "\n",
    "The only black box we will use for our fits is the ancient but proven [MINUIT](https://en.wikipedia.org/wiki/MINUIT) minimization package (which is also used by [ROOT](https://root.cern.ch/)). I particular, we use it's python wrapper [iminuit](https://github.com/scikit-hep/iminuit). Of course there are other minimation package in python that might be even more wide-spread (the most famous one is [scipy.optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html), which wraps around the Fortran code [MINPACK](https://en.wikipedia.org/wiki/MINPACK)).\n",
    "\n",
    "In the `minuit_minimize.py` file, I implemented a function that uses *iminuit* to minimize a function that takes a dictionary of parameters. Let's import it, and of couse *pandas* and *numpy* too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from geeksw.optimize.minuit_minimize import minuit_minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see how that function can be used, run `?minuit_minimize` or `??minuit_minimize` to inspect the documentation string or source code.\n",
    "\n",
    "Next, we summarize our results in a data frame that we will then pass to some fitting function.\n",
    "In this example, we will use some values from an outdated version of the CMS analysis, which also includes more final states than just the four leptons.\n",
    "\n",
    "The rows will correspond to the category bins that we have in the analysis. In this example, the bins are the  different lepton multiplicities in the final state (`SSplusJets` means two same sign leptons plus jets and is used in the *WWW* analysis).\n",
    "\n",
    "The columns are the different components that we have in the simulation. At this point it doesn't matter how the backgrounds are composed, and we just have one background component `B` in addition to four on-shell *VVV* and four *VH* to *VVV* components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacard = pd.DataFrame(dict(WWW=[27.90, 16.07, 0, 0, 0],\n",
    "                             WWZ=[0.91, 0.91, 8.57, 0, 0],\n",
    "                             WZZ=[0.18, 0.01, 0.34, 1.08, 0.01],\n",
    "                             ZZZ=[0, 0, 0.07, 0.25, 0.41],\n",
    "                             VH_WWW=[17.62, 8.47, 0, 0, 0],\n",
    "                             VH_WWZ=[0, 0,  5.6+0.09+0.17, 0, 0],\n",
    "                             VH_WZZ=[0.0, 0, 0, 0, 0],\n",
    "                             VH_ZZZ=[0.0, 0, 0, 0, 0],\n",
    "                             B=[447.21, 94.99, 15.76, 0.55, 0.06]),\n",
    "                        index=[\"SSplusJets\", \"3leptons\", \"4leptons\", \"5leptons\", \"6leptons\"])\n",
    "datacard.index.name = \"bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the data frame, which we calll \"datacard\", because that's how we call these kind of summary tables that we put into fitting tools like *Higgs combine* inside CMS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WWW</th>\n",
       "      <th>WWZ</th>\n",
       "      <th>WZZ</th>\n",
       "      <th>ZZZ</th>\n",
       "      <th>VH_WWW</th>\n",
       "      <th>VH_WWZ</th>\n",
       "      <th>VH_WZZ</th>\n",
       "      <th>VH_ZZZ</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SSplusJets</th>\n",
       "      <td>27.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>447.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3leptons</th>\n",
       "      <td>16.07</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4leptons</th>\n",
       "      <td>0.00</td>\n",
       "      <td>8.57</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5leptons</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6leptons</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              WWW   WWZ   WZZ   ZZZ  VH_WWW  VH_WWZ  VH_WZZ  VH_ZZZ       B\n",
       "bin                                                                        \n",
       "SSplusJets  27.90  0.91  0.18  0.00   17.62    0.00     0.0     0.0  447.21\n",
       "3leptons    16.07  0.91  0.01  0.00    8.47    0.00     0.0     0.0   94.99\n",
       "4leptons     0.00  8.57  0.34  0.07    0.00    5.86     0.0     0.0   15.76\n",
       "5leptons     0.00  0.00  1.08  0.25    0.00    0.00     0.0     0.0    0.55\n",
       "6leptons     0.00  0.00  0.01  0.41    0.00    0.00     0.0     0.0    0.06"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datacard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it will get interesting. We want to use the recommended statistical analysis for LHC experiments described in [this paper](https://arxiv.org/pdf/1503.07622.pdf), in particular Chapter 5 about frequentist statistical procedures.\n",
    "\n",
    "In a discovery setting where we don't know a priori the signal strength $\\mu$ (the ratio of observed over expected signal), we compute the likelihood ratio to test the background hypothesis:\n",
    "\n",
    "$$ \\lambda(0) = \\frac{L(0, \\hat{\\hat{\\theta}}(0))}{L(\\hat{\\mu}, \\hat{\\theta})}. $$\n",
    "\n",
    "This corresponds to Equation 49 in the paper with the expected signal $\\mu = 0$. Next, we compute the discovery test statistic $q_0$ like in Equation 51:\n",
    "\n",
    "$$q_0 = -2\\log{\\lambda(0)}.$$\n",
    "\n",
    "Finally, we use the asymptotic formula from Equation 72 to get the significance:\n",
    "\n",
    "$$Z = \\sqrt{q_0}.$$\n",
    "\n",
    "In the following, I show you my implementation of the LHC discovery test. It starts with some imports and a little helper function which is not important to understand now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "import scipy.special\n",
    "\n",
    "def get_nuisance_parameter_name(func):\n",
    "    args = inspect.getfullargspec(func).args\n",
    "    assert(len(args) == 2)\n",
    "    assert(args[1] == \"datacard\")\n",
    "    return args[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I have a function that builds **negative log-likelihood functions** which you have to pass:\n",
    "* a datacard\n",
    "* the definition of signal strengths we want to measure\n",
    "* a list of nuisance parameter functions and penalties (we will cover this later)\n",
    "* the number of observed events in data if you want to compute the observed significance (if this is `None` you will get the expected significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nll_func(datacard, signal_strengths, nuisances=[], observations=None):\n",
    "        \n",
    "    if observations is None:\n",
    "        observations = datacard.sum(axis=1)\n",
    "    \n",
    "    # This is the negative logarithm of the Possonian probabiliy density to observe n events\n",
    "    # with an expectation s + b\n",
    "    def nll_term(s, b, n):\n",
    "        return (s+b) - n * np.log(s+b)\n",
    "    \n",
    "    signal_columns = \"+\".join(signal_strengths.values()).split(\"+\")\n",
    "    background_columns = [c for c in datacard.columns if not c in signal_columns]\n",
    "\n",
    "    nuisance_param_names = [get_nuisance_parameter_name(nuis) for nuis in nuisances]\n",
    "    \n",
    "    def nll_func(**params):\n",
    "                \n",
    "        df = datacard.copy(deep=True)\n",
    "        \n",
    "        nll = 0.0\n",
    "\n",
    "        for nuisance, param_name in zip(nuisances, nuisance_param_names):\n",
    "            df, constraint = nuisance(params[param_name], df)\n",
    "            nll += constraint\n",
    "        \n",
    "        s = pd.DataFrame({key : params[key] * \\\n",
    "                          df.eval(signal_strengths[key]) for key in signal_strengths}).sum(axis=1)\n",
    "        b = df[background_columns].sum(axis=1)\n",
    "    \n",
    "        return nll + nll_term(s, b, observations).sum()\n",
    "    \n",
    "    return nll_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the full **negative log-likelihood** function for you analysis is very valuable. Next, we will use it to compute discovery significances in this `asymptotic_discovery_fit` function. It takes the same arguments as `make_nll_func` but additionally **iminuit** to find optimal parameter values and execute the LHC discovery statistic prescription.\n",
    "\n",
    "In the resulting tuple, you will have one data frame which shows you the fitted signal strengths and discovery significances, and one data frame which summarizes the nuisance parameter fit result (not important for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymptotic_discovery_fit(datacard, signal_strengths=None, nuisances=[], observations=None):\n",
    "\n",
    "    nll_func = make_nll_func(datacard, signal_strengths, nuisances, observations)\n",
    " \n",
    "    nuisance_param_names = [get_nuisance_parameter_name(nuis) for nuis in nuisances]\n",
    "    signal_strength_param_names = list(signal_strengths.keys())\n",
    "    param_names = signal_strength_param_names + nuisance_param_names\n",
    "    n_params = len(param_names)\n",
    "    starting_params = [1.1] * n_params\n",
    "    starting_errors = [0.1] * n_params\n",
    "    \n",
    "    values, errors, _ = minuit_minimize(nll_func, param_names, starting_params, starting_errors)\n",
    "    \n",
    "    df = pd.DataFrame(dict(value=list(values.values()),\n",
    "                           error=list(errors.values())), index=param_names)\n",
    "    df = df.eval(\"precision=error/value\")\n",
    "    \n",
    "    def set_elem_to_zero(a,  name):\n",
    "        b = dict(**a)\n",
    "        b[name] = 0.0\n",
    "        return b\n",
    "    \n",
    "    def two_dnnl(name):\n",
    "        fixed_params = {name : 0.0}\n",
    "        if name in fixed_params and len(param_names) == 1:\n",
    "            null_nll = nll_func(**fixed_params)\n",
    "        else:\n",
    "            null_nll = minuit_minimize(nll_func, param_names, starting_params, starting_errors,\n",
    "                                       fixed_params=fixed_params).f_val\n",
    "        ll_0 = -null_nll\n",
    "        ll_best_fit = -nll_func(**values)\n",
    "        return -2 * (ll_0 - ll_best_fit)\n",
    "\n",
    "    df_mu = df.loc[signal_strength_param_names]\n",
    "    df_nuisance = df.loc[nuisance_param_names]\n",
    "    \n",
    "    df_mu[\"significance\"] = np.sqrt(np.array(list(map(two_dnnl, signal_strength_param_names))))\n",
    "    df_mu[\"p_value\"] = 0.5 * (1.+scipy.special.erf(-df_mu[\"significance\"]/np.sqrt(2.)))\n",
    "    \n",
    "    return df_mu, df_nuisance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show you how to use this function. The **signal strength** definitions are just a dictionary of data frame queries, and each entry is a separate signal for the analysis. Therefore, we can very quickly try out different signal definitions.\n",
    "\n",
    "For example, taking the sum of all triboson final states as one single signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_strengths = dict(mu_all=\"WWW+WWZ+WZZ+ZZZ+VH_WWW+VH_WWZ+VH_WZZ+VH_ZZZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or consider only on-shell production and have separate signal strengths for different final states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_strengths = dict(mu_WWW=\"WWW\", mu_WWZ=\"WWZ\", mu_WZZ=\"WZZ\", mu_ZZZ=\"ZZZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the datacard and the signal strength definition to the function and save the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mu, df_nuisance = asymptotic_discovery_fit(datacard, signal_strengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the results (`df_nuisance` is empty if you didn't pass nuisance parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>error</th>\n",
       "      <th>precision</th>\n",
       "      <th>significance</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu_WWW</th>\n",
       "      <td>1.001083</td>\n",
       "      <td>0.519374</td>\n",
       "      <td>0.518812</td>\n",
       "      <td>1.994590</td>\n",
       "      <td>0.023044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_WWZ</th>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.647161</td>\n",
       "      <td>0.647711</td>\n",
       "      <td>1.714708</td>\n",
       "      <td>0.043199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_WZZ</th>\n",
       "      <td>0.998764</td>\n",
       "      <td>1.329986</td>\n",
       "      <td>1.331632</td>\n",
       "      <td>0.864676</td>\n",
       "      <td>0.193608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_ZZZ</th>\n",
       "      <td>1.002636</td>\n",
       "      <td>1.695322</td>\n",
       "      <td>1.690865</td>\n",
       "      <td>0.998224</td>\n",
       "      <td>0.159085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           value     error  precision  significance   p_value\n",
       "mu_WWW  1.001083  0.519374   0.518812      1.994590  0.023044\n",
       "mu_WWZ  0.999151  0.647161   0.647711      1.714708  0.043199\n",
       "mu_WZZ  0.998764  1.329986   1.331632      0.864676  0.193608\n",
       "mu_ZZZ  1.002636  1.695322   1.690865      0.998224  0.159085"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a nice tool to follow the LHC discovery test! We will later learn how to consider systematic uncertainties as well.\n",
    "\n",
    "Here is what you should do with these new tools:\n",
    "\n",
    "1. Do the discovery test for your four-lepton analyis and compare the results to what you get with the AMS added in quadrature. Do your results agree?\n",
    "2. Try out different signal definitions, i.e. *VH* included or not included\n",
    "3. Compare the significance you obtain with *VVV+VH_VVV* as a signal with the significances for *VVV* and *VH_VVV* added separately in quadrature. Would you expect the significances to be the same? Explain what you observe.\n",
    "4. By adapting the tools in this notebook, you can also create likelihood scans in 1D and 2D. This would be the next step for next week."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
